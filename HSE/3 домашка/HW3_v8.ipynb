{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3v5HIUdDvY5"
   },
   "source": [
    "# HSE 2024: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 3\n",
    "\n",
    "**Warning 1**: some problems require (especially the lemmatization part) significant amount of time, so **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F7t9dYtdDvZC",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:50.388045Z",
     "start_time": "2024-11-10T04:36:50.323399Z"
    }
   },
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIHtwV6vDvZD"
   },
   "source": [
    "## PART 1: Logit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7XKEWcVDvZD"
   },
   "source": [
    "We consider a binary classification problem. For prediction, we would like to use a logistic regression model. For regularization we add a combination of the $l_2$ and $l_1$ penalties (Elastic Net).\n",
    "\n",
    "Each object in the training dataset is indexed with $i$ and described by pair: features $x_i\\in\\mathbb{R}^{K}$ and binary labels $y_i$. The model parametrized with bias $w_0\\in\\mathbb{R}$ and weights $w\\in\\mathbb{R}^K$. Note: Bias is included in $w$ vector\n",
    "\n",
    "The optimization problem with respect to the $w_0, w$ is the following (Logistic loss with Elastic Net regularizers):\n",
    "\n",
    "$$L(w, w_0) = \\sum_{i=1}^{N} -y_i \\log{\\sigma{(w^\\top x_i)}} - (1 - y_i) \\log{(1 - \\sigma{(w^\\top x_i)})} + \\gamma \\|w\\|_1 + \\beta \\|w\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1eSuDKXFVZu"
   },
   "source": [
    "#### 1. [0.5 points]  Find the gradient of the Elastic Net loss and write its formulas (better in latex format). Remember what derivative sigmoid has (gradient in fact is a lot simpler than you may get using automatic tools like sympy, matlab or whatever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zjH-YnPDvZD"
   },
   "source": [
    "Градиент для Elastic Net Loss равен:\n",
    "\n",
    "$$ \\nabla_w L(w, w_0) = \\sum_{i=1}^{N}(\\sigma{(w^\\top x_i)} - y_i)x_i + \\gamma_1 \\, k + 2 \\beta \\|w\\|_1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_lIccN_DvZE"
   },
   "source": [
    "#### 2. [0.25 points] Implement the Elastic Net loss (as a function)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9QNfCtV5DvZE",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:50.593693Z",
     "start_time": "2024-11-10T04:36:50.577158Z"
    }
   },
   "source": [
    "def loss(X, y, w: List[float], gamma=1., beta=1.) -> float:\n",
    "    w = np.array(w)\n",
    "    pred = w.T * X\n",
    "    sig = 1 / (1 + np.exp(-pred))\n",
    "    \n",
    "    main_loss = np.sum(-y * np.log(sig) - (1 - y) * np.log(1 - sig))\n",
    "    \n",
    "    l1 = gamma * np.sum(np.abs(w))\n",
    "    \n",
    "    l2 = beta * np.sum(w ** 2)\n",
    "    \n",
    "    final_loss = main_loss + l1 + l2\n",
    "    return final_loss"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIVoC6UmDvZE"
   },
   "source": [
    "#### 3. [0.25 points] Implement the gradient (as a function)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HWqBLGRADvZE",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:50.627608Z",
     "start_time": "2024-11-10T04:36:50.606844Z"
    }
   },
   "source": [
    "def get_grad(X, y, w: List[float], gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
    "    w = np.array(w)\n",
    "    \n",
    "    w = np.array(w)\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    pred = w.T * X\n",
    "    \n",
    "    sig = 1 / (1 + np.exp(-pred))\n",
    "    \n",
    "    error = sig - y\n",
    "    grad_logistic = error * X\n",
    "\n",
    "    grad_l1 = gamma * np.sign(w)\n",
    "\n",
    "    grad_l2 = 2 * beta * w\n",
    "\n",
    "    grad_w = grad_logistic + grad_l1 + grad_l2\n",
    "    \n",
    "    return grad_w.tolist()\n",
    "    \n",
    "\n",
    "    return grad_w"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhOb8HrtDvZF"
   },
   "source": [
    "#### Check yourself"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3FxXTocHDvZF",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:50.966715Z",
     "start_time": "2024-11-10T04:36:50.678629Z"
    }
   },
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.multivariate_normal(np.arange(5), np.eye(5), size=10)\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "y = np.random.binomial(1, 0.42, size=10)\n",
    "w = np.random.normal(size=5 + 1)\n",
    "\n",
    "\n",
    "grad_w = get_grad(X, y, w)\n",
    "print(grad_w)\n",
    "assert(np.allclose(grad_w,\n",
    "                   [-3.99447493, -1.84786723,  0.64520104,  1.67059973, -5.03858487, -5.21496336],\n",
    "                   rtol=1e-2)\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,6) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mbinomial(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0.42\u001B[39m, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m      5\u001B[0m w \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal(size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m grad_w \u001B[38;5;241m=\u001B[39m get_grad(X, y, w)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(grad_w)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m(np\u001B[38;5;241m.\u001B[39mallclose(grad_w,\n\u001B[0;32m     11\u001B[0m                    [\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3.99447493\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1.84786723\u001B[39m,  \u001B[38;5;241m0.64520104\u001B[39m,  \u001B[38;5;241m1.67059973\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m5.03858487\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m5.21496336\u001B[39m],\n\u001B[0;32m     12\u001B[0m                    rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m)\n\u001B[0;32m     13\u001B[0m )\n",
      "Cell \u001B[1;32mIn[7], line 12\u001B[0m, in \u001B[0;36mget_grad\u001B[1;34m(X, y, w, gamma, beta)\u001B[0m\n\u001B[0;32m      8\u001B[0m pred \u001B[38;5;241m=\u001B[39m w\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m*\u001B[39m X\n\u001B[0;32m     10\u001B[0m sig \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39mpred))\n\u001B[1;32m---> 12\u001B[0m error \u001B[38;5;241m=\u001B[39m sig \u001B[38;5;241m-\u001B[39m y\n\u001B[0;32m     13\u001B[0m grad_logistic \u001B[38;5;241m=\u001B[39m error \u001B[38;5;241m*\u001B[39m X\n\u001B[0;32m     15\u001B[0m grad_l1 \u001B[38;5;241m=\u001B[39m gamma \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msign(w)\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (10,6) (10,) "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbqLfcrRDvZF"
   },
   "source": [
    "####  4. [1 point]  Implement gradient descent which works for both tol level and max_iter stop criteria and plot the decision boundary of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIgiwQkjDvZF"
   },
   "source": [
    "The template provides basic sklearn API class. You are free to modify it in any convenient way."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Thyeux0KDvZG",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:50.971904Z",
     "start_time": "2024-11-10T04:36:50.971904Z"
    }
   },
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class Logit(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, beta=1.0, gamma=1.0, lr=1e-3, tolerance=0.01, max_iter=1000, random_state=42):\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tolerance= tolerance\n",
    "        self.max_iter= max_iter\n",
    "        self.learning_rate = lr\n",
    "        self.random_state = random_state\n",
    "        self.w = None\n",
    "        # you may additional properties if you wish\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # add weights and bias and optimize Elastic Net loss over (X,y) dataset\n",
    "        # save history of optimization steps\n",
    "\n",
    "        # your code here\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # return vector of predicted labels (0 or 1) for each object from X\n",
    "        # your code here\n",
    "\n",
    "        return predict\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "      # return vector of probabilities for each object from X\n",
    "        return np.array([1 / (1 + np.exp(np.dot(X, self.w))),\\\n",
    "                         1 / (1 + np.exp(-np.dot(X, self.w)))])"
   ],
   "metadata": {
    "id": "4I-mPUA6YaEH",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:50.973435Z",
     "start_time": "2024-11-10T04:36:50.973435Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7SJX8Y6EDvZG"
   },
   "source": [
    "# sample data to test your model\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=180, n_features=2, n_redundant=0, n_informative=2,\n",
    "                               random_state=42, n_clusters_per_class=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u41kzwGTDvZH",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:51.033386Z",
     "start_time": "2024-11-10T04:36:51.014552Z"
    }
   },
   "source": [
    "# a function to plot the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    fig = plt.figure()\n",
    "    X1min, X2min = X.min(axis=0)\n",
    "    X1max, X2max = X.max(axis=0)\n",
    "    x1, x2 = np.meshgrid(np.linspace(X1min, X1max, 200),\n",
    "                         np.linspace(X2min, X2max, 200))\n",
    "    ypred = model.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "    ypred = ypred.reshape(x1.shape)\n",
    "\n",
    "    plt.contourf(x1, x2, ypred, alpha=.4)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mNuYbsAoDvZI",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:51.174642Z",
     "start_time": "2024-11-10T04:36:51.104390Z"
    }
   },
   "source": [
    "model = Logit(0, 0)\n",
    "model.fit(X, y)\n",
    "plot_decision_boundary(model, X, y)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Logit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m Logit(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[0;32m      3\u001B[0m plot_decision_boundary(model, X, y)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Logit' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi4WRhcADvZI"
   },
   "source": [
    "#### 5. [0.25 points] Plot loss diagram for the model, i.e. show the dependence of the loss function from the gradient descent steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VyjMDAKuDvZI",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:51.436637Z",
     "start_time": "2024-11-10T04:36:51.425884Z"
    }
   },
   "source": [
    "# your code here"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FhSCAv_DvZJ"
   },
   "source": [
    "## PART 2: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYyGsSxEDvZJ"
   },
   "source": [
    "#### 6. [2 point] Using the same dataset, train SVM Classifier from Sklearn.\n",
    "Investigate how different parameters influence the quality of the solution:\n",
    "+ Try several kernels: Linear, Polynomial, RBF (and others if you wish). Some Kernels have hypermeters: don't forget to try different.\n",
    "+ Regularization coefficient\n",
    "\n",
    "Show how these parameters affect accuracy, roc_auc and f1 score.\n",
    "Make plots for the dependencies between metrics and parameters.\n",
    "Try to formulate conclusions from the observations. How sensitive are kernels to hyperparameters? How sensitive is a solution to the regularization? Which kernel is prone to overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nicu_O3IDvZK",
    "ExecuteTime": {
     "end_time": "2024-11-10T04:36:51.514703Z",
     "start_time": "2024-11-10T04:36:51.505017Z"
    }
   },
   "source": [
    "# your code here"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY8q6JdCDvZK"
   },
   "source": [
    "## PART 3: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD4xKhYfDvZK"
   },
   "source": [
    "#### 7. [1.75 point] Form the dataset\n",
    "\n",
    "We are going to form a dataset that we will use in the following tasks for binary and multiclass classification\n",
    "\n",
    "0. Choose **six** authors that you like (specify who you've chosen) and download the <a href=\"https://www.kaggle.com/d0rj3228/russian-literature?select=prose\">relevant data</a> from **prose** section\n",
    "1. Build your own dataset for these authors:\n",
    "    * divide each text into sentences such that we will have two columns: *sentence* and *target author*, each row will contain one sentence and one target\n",
    "    * drop sentences where N symbols in a sentence < 15\n",
    "    * fix random state and randomly choose sentences in the folowing proportion \"5k : 15k : 8k : 11k : 20k : 3k\" for the authors respectively\n",
    "    \n",
    "    sample data may look like:\n",
    "    \n",
    "    <center>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th> sentence </th>\n",
    "            <th> author </th>\n",
    "        </tr>\n",
    "        <tr><td> Несколько лет тому назад в одном из своих поместий жил старинный русской барин, Кирила Петрович Троекуров. </td><td> Пушкин </td><td>\n",
    "        <tr><td> Уже более недели приезжий господин жил в городе, разъезжая по вечеринкам и обедам и таким образом проводя, как говорится, очень приятно время. </td><td> Гоголь </td><td>\n",
    "        <tr><td> ... </td><td> ... </td><td>\n",
    "        <tr><td> Я жил недорослем, гоняя голубей и играя в чехарду с дворовыми мальчишками. </td><td> Пушкин </td><td>         \n",
    "    </table>\n",
    "</center>\n",
    "     \n",
    "2. Preprocess (tokenize and clean) the dataset\n",
    "    * tokenize, remove all stop words (nltk.corpus.stopwords), punctuation (string.punctuation) and numbers\n",
    "    * convert to lower case and apply either stemming or lemmatization of the words (on your choice)\n",
    "    * vectorize words using both **bag of words** and **tf-idf** (use sklearn)\n",
    "    * observe and describe the difference between vectorized output (what do numbers look like after transformations and what do they represent?)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NVStbeQ8DvZL",
    "ExecuteTime": {
     "end_time": "2024-11-10T05:34:50.387220Z",
     "start_time": "2024-11-10T05:34:27.622105Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('punkt')  # потребуется загрузить punkt, если не установлен\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 0\n",
    "authors = ['Blok', 'Bryusov', 'Chekhov', 'Dostoevsky', 'Gogol', 'Gorky']\n",
    "\n",
    "# 1\n",
    "path = 'data/prose/'\n",
    "\n",
    "sentences = []\n",
    "for i in authors:\n",
    "    folder_path = Path(path + i)\n",
    "    for file_path in folder_path.glob('*.txt'):\n",
    "        with open(file_path, encoding = 'utf-8') as f:\n",
    "            text = f.read().replace('\\n', ' ')\n",
    "            split_text = sent_tokenize(text)\n",
    "            for sentence in split_text:\n",
    "                if len(sentence) > 15:\n",
    "                    sentences.append({\"sentence\": sentence.strip(), \"author\": i})\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences)\n",
    "sentences_df.head(10)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                            sentence author\n",
       "0  Ни сны, ни явь     Мы сидели на закате всем се...   Blok\n",
       "1        За сиренями из оврага уже поднимался туман.   Blok\n",
       "2                      Стало слышно, как точат косы.   Blok\n",
       "3       Соседние мужики вышли косить купеческий луг.   Blok\n",
       "4                  Не орут, не ругаются, как всегда.   Blok\n",
       "5   Косы зашаркали по траве, слышно - штук двадцать.   Blok\n",
       "6                     Вдруг один из них завел песню.   Blok\n",
       "7  Без усилия полился и сразу наполнил и овраг, и...   Blok\n",
       "8  За сиренью, за туманом ничего не разглядеть, п...   Blok\n",
       "9                           Мужики подхватили песню.   Blok"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ни сны, ни явь     Мы сидели на закате всем се...</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>За сиренями из оврага уже поднимался туман.</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Стало слышно, как точат косы.</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Соседние мужики вышли косить купеческий луг.</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Не орут, не ругаются, как всегда.</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Косы зашаркали по траве, слышно - штук двадцать.</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Вдруг один из них завел песню.</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Без усилия полился и сразу наполнил и овраг, и...</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>За сиренью, за туманом ничего не разглядеть, п...</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Мужики подхватили песню.</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T06:15:15.971567Z",
     "start_time": "2024-11-10T06:15:15.627499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(sentences_df.shape)\n",
    "\n",
    "\n",
    "sample_sizes = [5000, 15000, 8000, 11000, 20000, 3000]\n",
    "sampled_sentences = pd.DataFrame()\n",
    "\n",
    "for author, size in zip(authors, sample_sizes):\n",
    "    author_sentences = sentences_df[sentences_df['author'] == author]\n",
    "    sampled_sentences = pd.concat([sampled_sentences, author_sentences.sample(n=size, random_state=42, replace=True)])\n",
    "\n",
    "shuffled_df = sampled_sentences.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_df.head(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225859, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                            sentence   author\n",
       "0   124        Господь не изменял, да Никон изменил.    Gorky\n",
       "1  Он жадно всматривался в лица, слушал не мигая,...  Chekhov\n",
       "2  Уже тонкие чары темной женщины не давали ему п...     Blok\n",
       "3  – Затем начал он слегка поворачивать бричку, п...    Gogol\n",
       "4              Не ночевать же в такое время в степи.    Gogol\n",
       "5  ступай в кладовую, вынь ковер самый лучший — ч...    Gogol\n",
       "6  — от каких-нибудь негодных клопов, которым бы ...    Gogol\n",
       "7  Мудрость обладает силой побеждать все бедствия...  Bryusov\n",
       "8  Он вспомнил вопросы, которые задавали ему, пок...  Chekhov\n",
       "9  Я рассказываю теми краткими словами, как расск...    Gorky"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124        Господь не изменял, да Никон изменил.</td>\n",
       "      <td>Gorky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Он жадно всматривался в лица, слушал не мигая,...</td>\n",
       "      <td>Chekhov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Уже тонкие чары темной женщины не давали ему п...</td>\n",
       "      <td>Blok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>– Затем начал он слегка поворачивать бричку, п...</td>\n",
       "      <td>Gogol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Не ночевать же в такое время в степи.</td>\n",
       "      <td>Gogol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ступай в кладовую, вынь ковер самый лучший — ч...</td>\n",
       "      <td>Gogol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>— от каких-нибудь негодных клопов, которым бы ...</td>\n",
       "      <td>Gogol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Мудрость обладает силой побеждать все бедствия...</td>\n",
       "      <td>Bryusov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Он вспомнил вопросы, которые задавали ему, пок...</td>\n",
       "      <td>Chekhov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Я рассказываю теми краткими словами, как расск...</td>\n",
       "      <td>Gorky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T08:45:01.259655Z",
     "start_time": "2024-11-10T08:44:57.178074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "import string\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(words)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T08:53:14.543290Z",
     "start_time": "2024-11-10T08:52:50.937070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = shuffled_df\n",
    "df['cleaned_text'] = df['sentence'].apply(preprocess_text)\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T08:53:55.795438Z",
     "start_time": "2024-11-10T08:53:53.759265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_vectors = bow_vectorizer.fit_transform(df['cleaned_text'])\n"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T08:54:19.833530Z",
     "start_time": "2024-11-10T08:54:17.454265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XZUP1HqFDvZL"
   },
   "source": [
    "# your code here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Binary classification\n",
    "\n",
    "#### 8. [2 point] Train model using Logistic Regression (your own) and SVC (SVM can be taken from sklearn)\n",
    "\n",
    "* choose *two* authors from the dataset that you have formed in the previous task\n",
    "* check the balance of the classes\n",
    "* divide the data into train and test samples with 0.7 split rate (don't forget to fix the random state)\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score) and use it in the next tasks\n",
    "* make several plots to address the dependence between F1 score and parameters\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute some relevant metrics for test sample (useful to check the seminars 5 and 6, use sklearn)\n",
    "* make conclusions about the performance of your models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G1kt0qbDvZL"
   },
   "source": [
    "#### 9. [1 point] Analysing ROC AUC\n",
    "\n",
    "It is possible to control the proportion of statistical errors of different types using different thresholds for choosing a class. Plot ROC curves for Logistic Regression and SVC, show the threshold on ROC curve plots. Choose such a threshold that your models have no more than 30% of false positive errors rate. Pay attention to `thresholds` parameter in sklearn roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BZ2GZ8-uDvZL"
   },
   "source": [
    "# your code here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-qubaK4DvZM"
   },
   "source": [
    "### Multiclass logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJQone-LDvZM"
   },
   "source": [
    "#### 10. [1 point] Take the One-VS-One classifier (use sklearn) and apply to Logit model (one you've made in the 4th task) in order to get multiclass linear classifier\n",
    "\n",
    "*It is possible to use sklearn model instead of your own one but with a penalty of 0.5*\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html\">OneVsOneClassifier</a>\n",
    "\n",
    "* use the data you got at the previous step for 6 authors\n",
    "* divide the data into train and test samples with 0.7 split rate\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score)\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute relevant metrics for test sample (use sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M4lR8qJ7DvZM"
   },
   "source": [
    "# your code here"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW3_v7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
